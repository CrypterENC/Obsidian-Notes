# Beautiful Soup for Web Scraping

## Introduction

- Beautiful Soup is a Python library for parsing HTML and XML documents.
- It builds a parse tree from the HTML source, making it easy to navigate and extract data.
- Commonly used in web scraping after fetching pages with libraries like `requests`.
- In security scripting, useful for extracting structured data from web responses.

## Installation

- Install via pip: `pip install beautifulsoup4`
- Optional: Install lxml parser for better performance: `pip install lxml`
- Use 'html.parser' (built-in) or 'lxml' as the parser.

## Basic Usage

- Import the library: `from bs4 import BeautifulSoup`
- Create a soup object: `soup = BeautifulSoup(html_string, 'html.parser')`
- The soup object allows searching and navigating the HTML tree.

## Key Methods

- `soup.find(tag, attrs={})`: Finds the first element matching the tag and attributes.
- `soup.find_all(tag, attrs={})`: Finds all elements matching the criteria.
- `element.text`: Extracts the text content inside the element (without tags).
- `element.get('attribute')`: Retrieves the value of a specific attribute (e.g., 'href', 'src').
- Navigation: Access parent (`element.parent`), children (`element.children`), siblings (`element.next_sibling`), etc.
- CSS Selectors: `soup.select('css_selector')` for advanced queries using CSS syntax.

## Examples

### Basic Example: Extract Title and Links

```python
import requests
from bs4 import BeautifulSoup

# Fetch the page
url = 'https://example.com'
response = requests.get(url)
response.raise_for_status()  # Ensure successful response

# Parse HTML
soup = BeautifulSoup(response.text, 'html.parser')

# Extract title
title = soup.find('title')
if title:
    print(f"Page Title: {title.text}")
else:
    print("No title found")

# Extract all links
links = soup.find_all('a')
for link in links:
    href = link.get('href')
    if href:
        print(f"Link: {href}")
```

### Advanced Example: Find Elements by Class

```python
# Find all divs with class 'article'
articles = soup.find_all('div', class_='article')

for article in articles:
    # Extract heading
    heading = article.find('h2')
    if heading:
        print(f"Heading: {heading.text}")
    
    # Extract paragraph text
    para = article.find('p')
    if para:
        print(f"Content: {para.text}")
```

### Using CSS Selectors

```python
# Select all links inside a specific div
selected_links = soup.select('div.content a')
for link in selected_links:
    print(link.get('href'))
```

## Notes

- Always check if elements exist before accessing `.text` or attributes to avoid `AttributeError`.
- Use `response.encoding` and `response.text` to handle character encoding properly.
- For dynamic content (JavaScript-generated), combine with Selenium or similar.
- In security contexts, parse HTML to extract data like forms, inputs, or hidden fields for further analysis.
- Respect website policies and avoid overloading servers; use delays between requests if scraping multiple pages.