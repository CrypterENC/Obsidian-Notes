# Web Scraping in Python

## Introduction
- Web scraping extracts data from websites automatically
- Useful for data collection, monitoring, research
- Respect website terms of service and robots.txt

## Basic Tools
- **Requests**: Fetch web pages
- **BeautifulSoup**: Parse HTML and extract data
- **Scrapy**: Framework for large-scale scraping
- **Selenium**: Handle dynamic content (JavaScript)

## Crawler
- A crawler (or spider) navigates the web by following links
- Used for discovering and indexing web pages
- Can be built recursively or with frameworks like Scrapy

### Example Crawler with Recursive Function
```python
import requests
from bs4 import BeautifulSoup
import time

visited = set()

def crawl(url, max_depth=2, depth=0):
    if depth > max_depth or url in visited:
        return
    visited.add(url)
    
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Extract links
        links = soup.find_all('a', href=True)
        for link in links:
            next_url = link['href']
            if next_url.startswith('http'):
                crawl(next_url, max_depth, depth + 1)
                time.sleep(1)  # Be polite
    except:
        pass

crawl('https://example.com')
```

## Scraper
- A scraper extracts specific data from web pages
- Focuses on parsing and collecting targeted information
- Often used on single pages or with crawlers

### Example Scraper
```python
import requests
from bs4 import BeautifulSoup

def scrape_product_info(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Example: Extract product title and price
    title = soup.find('h1').text.strip() if soup.find('h1') else 'No title'
    price = soup.find('span', class_='price').text.strip() if soup.find('span', class_='price') else 'No price'
    
    return {'title': title, 'price': price}

result = scrape_product_info('https://example.com/product')
print(result)
```

## Basic Example with BeautifulSoup
```python
import requests
from bs4 import BeautifulSoup

# Fetch the page
url = 'https://example.com'
response = requests.get(url)

# Parse HTML
soup = BeautifulSoup(response.text, 'html.parser')

# Extract data
title = soup.find('title').text
print(f'Title: {title}')

# Find elements by tag
links = soup.find_all('a')
for link in links:
    print(link.get('href'))
```

## Handling Dynamic Content with Selenium
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

# Setup webdriver
driver = webdriver.Chrome()  # Requires ChromeDriver

driver.get('https://example.com')

# Wait for elements to load
title = driver.find_element(By.TAG_NAME, 'title').text
print(f'Title: {title}')

driver.quit()
```

## Best Practices
- Use headers to mimic browser: `headers = {'User-Agent': 'Mozilla/5.0'}`
- Handle errors and retries
- Avoid overloading servers with delays
- Store data ethically
- Check for API alternatives

## Challenges in Web Scraping
- **Anti-Scraping Measures**: CAPTCHAs, rate limiting, IP blocking
- **Dynamic Content**: JavaScript-loaded content requires Selenium
- **Website Changes**: HTML structure changes break scrapers
- **Legal Issues**: Terms of service, copyright, privacy laws
- **Performance**: Resource-intensive, need to handle errors and retries
- **Ethical Concerns**: Respect robots.txt, avoid overloading servers

## Notes
- Websites may block scrapers; rotate user agents
- For large projects, use Scrapy framework
- Legal considerations: copyright, terms of use